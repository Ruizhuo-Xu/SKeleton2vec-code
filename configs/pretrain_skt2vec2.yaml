train_dataset:
  dataset:
    name: PoseDataset
    args:
      anno_file: data/nturgbd/ntu60_3danno.pkl
      pipelines: [
        {name: PreNormalize3D, args: {}},
        {name: RandomRot, args: {theta: 0.2}},
        {name: GenSkeFeat, args: {dataset: 'nturgb+d', feats:['j']}},
        {name: UniformSampleDecode, args: {clip_len: 120}},
        {name: FormatGCNInput, args: {}},
        {name: Collect, args: {keys: [keypoint, label], meta_keys: []}},
        {name: ToTensor, args: {keys: [keypoint, label]}},
      ]
      split: xsub_train
      # first_k: 100
  batch_size: 48
  num_workers: 8

val_dataset:
  dataset:
    name: PoseDataset
    args:
      anno_file: data/nturgbd/ntu60_3danno.pkl
      pipelines: [
        {name: PreNormalize3D, args: {}},
        {name: GenSkeFeat, args: {dataset: 'nturgb+d', feats:['j']}},
        {name: UniformSampleDecode, args: {clip_len: 120}},
        {name: FormatGCNInput, args: {}},
        {name: Collect, args: {keys: [keypoint, label], meta_keys: []}},
        {name: ToTensor, args: {keys: [keypoint, label]}},
      ]
      split: xsub_val
  batch_size: 8
  num_workers: 8

model:
  name: Skeleton2Vec2
  args:
    model_spec:
      name: SkTWithDecoder
      args: 
        in_channels: 3
        temporal_segment_size: 4
        spatio_size: 25
        temporal_size: 120
        encoder_emb_size: 256
        decoder_emb_size: 256
        encoder_depth: 8
        decoder_depth: 4
        num_heads: 8
        att_drop_p: 0.
        forward_drop_p: 0.
        drop_path_p: 0.2
        layer_scale_init_value: 1.e-4
        mask_strategy: 'tube'
    ema_spec:
      name: EMA
      args:
        ema_decay: 0.9998
        ema_end_decay: 0.99999
        ema_anneal_end_step: 20000
    average_top_k_layers: 8
    norm_target_per_layer: layer_norm
    normalize_targets: true
  
# resume: save/nturgbd60_pretrain_v2_tmask40_ep400/epoch-last.pth
# resume: save/nturgbd60_pretrain_skt2vec2_mask0.90_largeEMA_tube_resume/epoch-last.pth

optimizer:
  name: adamw
  args:
    lr: 5.e-4
    betas: [0.9, 0.95]
    weight_decay: 5.e-2

lr_scheduler:
  name: CosineDecayWithWarmup
  args:
    warmup_epochs: 20
    max_epochs: 400
    base_lr: 5.e-4
    min_lr: 1.e-5
    mode: step

mask_ratio: 0.85
tube_len: 10
num_masked_views: 1
grad_accum_steps: 1
# smooth_l1_beta: 1.
clip_grad: 3.0
epoch_max: 400
epoch_val: 1000
epoch_save: 20
