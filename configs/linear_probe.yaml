train_dataset:
  dataset:
    name: PoseDataset
    args:
      anno_file: data/nturgbd/ntu60_3danno.pkl
      pipelines: [
        {name: PreNormalize3D, args: {}},
        {name: RandomRot, args: {theta: 0.2}},
        {name: GenSkeFeat, args: {dataset: 'nturgb+d', feats:['j']}},
        # {name: UniformSampleDecode, args: {clip_len: 90}},
        {name: RandomResizedCrop, args: {p_interval: [0.5, 1.0], clip_len: 120}},
        {name: FormatGCNInput, args: {}},
        {name: Collect, args: {keys: [keypoint, label], meta_keys: []}},
        {name: ToTensor, args: {keys: [keypoint, label]}},
      ]
      split: xsub_train
      # first_k: 100
  batch_size: 64
  num_workers: 4

val_dataset:
  dataset:
    name: PoseDataset
    args:
      anno_file: data/nturgbd/ntu60_3danno.pkl
      pipelines: [
        {name: PreNormalize3D, args: {}},
        {name: GenSkeFeat, args: {dataset: 'nturgb+d', feats:['j']}},
        # {name: UniformSampleDecode, args: {clip_len: 90}},
        {name: RandomResizedCrop, args: {p_interval: [0.95, 0.95], clip_len: 120}},
        {name: FormatGCNInput, args: {}},
        {name: Collect, args: {keys: [keypoint, label], meta_keys: []}},
        {name: ToTensor, args: {keys: [keypoint, label]}},
      ]
      split: xsub_val
  batch_size: 64
  num_workers: 4

model:
  name: SkTForClassification
  args:
    encoder_spec:
      name: SkT
      args:
        in_channels: 3
        temporal_segment_size: 4
        spatio_size: 25
        temporal_size: 120
        emb_size: 256
        depth: 8
        num_heads: 8
        att_drop_p: 0.
        forward_drop_p: 0.
        drop_path_p: 0.
    cls_head_spec:
      name: ClassificationHeadLight
      args:
        n_classes: 60
        num_persons: 2
        drop_p: 0.
    encoder_pretrain_weight: save/nturgbd60_pretrain_skt2vec2_mask0.90_largeEMA_tube5_randomcrop/epoch-60.pth
    # encoder_pretrain_weight: save/nturgbd60_pretrain_skt2vec2_mask0.90_largeEMA_randomtube6/epoch-300.pth
    # encoder_pretrain_weight: save/nturgbd60_pretrain_skt2vec2_mask0.90_largeEMA_tube5/epoch-300.pth
    encoder_freeze: true
  
# resume: save/nturgbd60_pretrain_test_bz64-lr3e-4-mask0.8-wd1e-2-L2/epoch-last.pth

optimizer:
  name: sgd
  args:
    lr: 0.1
    momentum: 0.9
    weight_decay: 0.
epoch_max: 100

# lr_scheduler:
#   name: CosineDecayWithWarmup
#   args:
#     warmup_epochs: 0
#     max_epochs: 100
#     base_lr: 0.1
#     min_lr: 0.
#     mode: step
lr_scheduler:
  name: MultiStepLr
  args:
    milestones: [10, 30, 50]
    gamma: 0.1
    

label_smoothing: 0.
epoch_val: 1
epoch_save: 200
mode: linear_probe
